{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoybxMMjmFJl"
      },
      "outputs": [],
      "source": [
        "# Install library tambahan jika diperlukan\n",
        "!pip install scikit-fuzzy pandas scikit-learn matplotlib numpy\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import skfuzzy as fuzz\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file CSV\n",
        "uploaded = files.upload()  # Pilih file CSV\n",
        "\n",
        "# Membaca data dari file CSV\n",
        "file_name = list(uploaded.keys())[0]\n",
        "data = pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan kolom target tersedia\n",
        "target_columns = ['TSS', 'pH', 'EC', 'TDS', 'CHLA']\n",
        "data = data[target_columns]\n",
        "\n",
        "# Mengisi nilai NaN dengan median\n",
        "data = data.fillna(data.median())\n",
        "\n",
        "# Standarisasi data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# PCA untuk menurunkan dimensi menjadi 2 komponen utama\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "data[['PCA1', 'PCA2']] = pca_result  # Menyimpan hasil PCA dalam dua kolom\n",
        "\n",
        "# Fungsi untuk menghitung WL Index\n",
        "def calculate_wl_index(u, cntr, data_points):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    num_clusters = len(cntr)\n",
        "\n",
        "    for i in range(num_clusters):\n",
        "        cluster_distances = np.linalg.norm(data_points - cntr[i], axis=1)**2\n",
        "        weighted_distances = np.sum((u[i] ** 2) * cluster_distances)\n",
        "        sum_membership = np.sum(u[i])\n",
        "        numerator += weighted_distances / sum_membership if sum_membership != 0 else 0\n",
        "\n",
        "    inter_cluster_distances = [np.linalg.norm(cntr[i] - cntr[j])**2 for i in range(num_clusters) for j in range(i + 1, num_clusters)]\n",
        "    if inter_cluster_distances:\n",
        "        min_distance = np.min(inter_cluster_distances)\n",
        "        median_distance = np.median(inter_cluster_distances)\n",
        "        denominator = min_distance + median_distance\n",
        "\n",
        "    return numerator / denominator if denominator != 0 else float('inf')\n",
        "\n",
        "# Fungsi untuk menghitung Fuzzy Silhouette Index\n",
        "def calculate_fsi(u, cntr, data_points, alpha=1):\n",
        "    \"\"\"\n",
        "    Menghitung Fuzzy Silhouette Index (FSI) berdasarkan rumus teoretis.\n",
        "\n",
        "    Parameters:\n",
        "    - u: matriks partisi fuzzy (shape: [n_cluster, n_data])\n",
        "    - cntr: array centroid (shape: [n_cluster, n_features])\n",
        "    - data_points: array data asli (shape: [n_data, n_features])\n",
        "    - alpha: parameter kontrol bobot (default = 1)\n",
        "\n",
        "    Returns:\n",
        "    - FSI (float)\n",
        "    \"\"\"\n",
        "    n_data = data_points.shape[0]\n",
        "    n_cluster = cntr.shape[0]\n",
        "    silhouette_scores = []\n",
        "    weights = []\n",
        "\n",
        "    for j in range(n_data):\n",
        "        # Hitung keanggotaan tertinggi dan kedua tertinggi\n",
        "        membership = u[:, j]\n",
        "        sorted_indices = np.argsort(membership)[::-1]  # descending\n",
        "        p = sorted_indices[0]\n",
        "        q = sorted_indices[1]\n",
        "\n",
        "        # Hitung a(i): jarak ke centroid klaster sendiri\n",
        "        a = np.linalg.norm(data_points[j] - cntr[p])\n",
        "\n",
        "        # Hitung b(i): jarak ke centroid klaster tetangga\n",
        "        b = np.linalg.norm(data_points[j] - cntr[q])\n",
        "\n",
        "        # Silhouette untuk titik j\n",
        "        if max(a, b) != 0:\n",
        "            s_j = (b - a) / max(a, b)\n",
        "        else:\n",
        "            s_j = 0\n",
        "\n",
        "        # Bobot berdasarkan beda keanggotaan\n",
        "        weight = (membership[p] - membership[q]) ** alpha\n",
        "\n",
        "        silhouette_scores.append(s_j * weight)\n",
        "        weights.append(weight)\n",
        "\n",
        "    fsi = np.sum(silhouette_scores) / np.sum(weights)\n",
        "    return fsi\n",
        "\n",
        "# Fuzzy C-Means\n",
        "for n_clusters in [2, 3, 4, 5]:\n",
        "    # Fuzzy C-Means clustering\n",
        "    cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
        "        data[['PCA1', 'PCA2']].T, n_clusters, 2, error=0.005, maxiter=1000, init=None\n",
        "    )\n",
        "    cluster_labels = np.argmax(u, axis=0)  # Klasterisasi berbasis derajat keanggotaan\n",
        "\n",
        "    # Simpan hasil klasterisasi\n",
        "    data[f'Cluster_{n_clusters}'] = cluster_labels\n",
        "\n",
        "    # Hitung jumlah data per klaster\n",
        "    print(f\"Jumlah data per klaster untuk {n_clusters} klaster:\")\n",
        "    for i in range(n_clusters):\n",
        "        cluster_size = np.sum(cluster_labels == i)\n",
        "        print(f\"  Cluster {i+1}: {cluster_size}\")\n",
        "\n",
        "    # Evaluasi Fuzzy C-Means\n",
        "    # Partition Coefficient (PC)\n",
        "    pc = np.mean(np.sum(u**2, axis=0))\n",
        "    # Partition Entropy (PE)\n",
        "    pe = -np.mean(np.sum(u * np.log(u), axis=0))\n",
        "    # Xie-Beni Index (XB)\n",
        "    min_dist = np.min([np.linalg.norm(cntr[i] - cntr[j]) for i in range(len(cntr)) for j in range(i + 1, len(cntr))])\n",
        "    distances = np.linalg.norm(data[['PCA1', 'PCA2']].values - cntr[:, None], axis=2)**2\n",
        "    xb = np.sum(u * distances) / (len(data) * min_dist**2)\n",
        "    # WL Index (menggantikan MDI)\n",
        "    wl_index = calculate_wl_index(u, cntr, data[['PCA1', 'PCA2']].values)\n",
        "    # Fuzzy Silhouette Index (FSI)\n",
        "    fsi = calculate_fsi(u, cntr, data[['PCA1', 'PCA2']].values, alpha=1)\n",
        "\n",
        "    # Output evaluasi\n",
        "    print(f\"Evaluasi untuk {n_clusters} Klaster:\")\n",
        "    print(f\"  Partition Coefficient (PC): {pc:.4f}\")\n",
        "    print(f\"  Partition Entropy (PE): {pe:.4f}\")\n",
        "    print(f\"  Xie-Beni Index (XB): {xb:.4f}\")\n",
        "    print(f\"  WL Index: {wl_index:.4f}\")\n",
        "    print(f\"  Fuzzy Silhouette Index (FSI): {fsi:.4f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "904_ouC9wWA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ubah nama kolom ke versi lebih pendek\n",
        "short_names = {\n",
        "    'CHLA': 'CHLA',\n",
        "    'EC': 'EC',\n",
        "    'DO': 'DO',\n",
        "    'MNDWI': 'MNDWI',\n",
        "    'PH': 'PH',\n",
        "    'TUR': 'TURB'\n",
        "}\n",
        "\n",
        "# Ambil target kolom dengan nama baru\n",
        "target_cols = list(short_names.values())\n",
        "\n",
        "# Simpan hasil FCM ke DataFrame\n",
        "df_scaled = pd.DataFrame(scaled_data, columns=short_names.keys()).rename(columns=short_names)\n",
        "df_scaled['Cluster'] = cluster_labels  # Tambahkan label klaster dari FCM\n",
        "\n",
        "# Ambil hanya klaster 0 dan 1\n",
        "cluster_means = df_scaled.groupby('Cluster').mean().loc[[0, 1]]\n",
        "\n",
        "# Radar Chart\n",
        "angles = np.linspace(0, 2 * np.pi, len(target_cols), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Tutup lingkaran radar plot\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "# Plot hanya klaster 0 dan 1\n",
        "for cluster in [0, 1]:\n",
        "    values = cluster_means.loc[cluster].tolist()\n",
        "    values += values[:1]  # Tutup lingkaran radar plot\n",
        "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'Klaster {cluster + 1}')\n",
        "    ax.fill(angles, values, alpha=0.25)\n",
        "\n",
        "# Konfigurasi tampilan radar chart tanpa angka\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(target_cols, fontsize=10)\n",
        "ax.set_yticklabels([])  # Hapus angka di sumbu\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jqEIX8k2AR1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil ke CSV\n",
        "output_file = 'hasil_fcm_pca_revisi1.csv'\n",
        "data.to_csv(output_file, index=False)\n",
        "print(f\"Hasil klasterisasi disimpan di {output_file}.\")\n",
        "# Memberikan link untuk mengunduh file CSV\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "lYpyCXvXqqjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan eigenvector dan explained variance\n",
        "print(\"Eigenvector (Komponen Utama):\")\n",
        "print(pca.components_)\n",
        "\n",
        "print(\"\\nExplained Variance:\")\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# Menampilkan total variance yang dijelaskan oleh komponen pertama\n",
        "print(\"\\nTotal Explained Variance by the First Component:\")\n",
        "print(np.sum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "gj5u72zEE3Lh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}